{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the data in a numpy array\nD = pd.read_csv('../input/creditcard.csv')\n#print(D.loc[0:5,'Class'])\nZ = D.values\nX = Z[:, 0:30]\nY = Z[:, 30]\nprint (X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\ncount_classes = pd.value_counts(D['Class'], sort = True)\ncount_classes.plot(kind = 'bar', rot=0)\nplt.title(\"Class Distribution\")\nLABELS = [\"Normal\", \"Fraud\"]\nplt.xticks(range(2), LABELS)\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Before going any further operations on data, let's right a function to calculate, true positives, \n# false positives, true negatives, precision, recall and F1 score\ndef calulate_precision_recall(y_test, y_pred):\n    tn = 0;\n    tp = 0;\n    fp = 0;\n    fn = 0;\n    for i in range(len(y_pred)):\n        if( y_test[i] == 0 and y_pred[i].round() == 0):\n            tn = tn+1\n        if( y_test[i] == 1 and y_pred[i].round() == 0):\n            fn = fn+1\n        if( y_test[i] == 1 and y_pred[i].round() == 1):\n            tp = tp+1\n        if( y_test[i] == 0 and y_pred[i].round() == 1):\n            fp = fp+1\n    prec=0\n    rec=0\n    F1=0\n    if(tp+fp>0):\n        prec= tp/(tp+fp)  \n    if(tp+fn>0):\n        rec = tp/(tp+fn)\n    if(prec+rec>0):\n        F1 = 2*prec*rec/(prec+rec)\n    print('True Positives: ' + str(tp))\n    print('True Negatives: ' + str(tn))\n    print('False Positives: ' + str(fp))\n    print('False Negatives: ' + str(fn))\n    print('Precision: ' + str(prec))\n    print('Recall: ' + str(rec))\n    print('F1 score: ' + str(F1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the data into training and test set\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nX_train, X_test, y_train, y_test = train_test_split(X, Y)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's add a neural network and test the efficiency."},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import regularizers\nmodel = keras.Sequential([\n    keras.layers.BatchNormalization(axis=-1, input_shape=(30,), momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', \n                                    beta_regularizer=None, gamma_regularizer=None, \n                                    beta_constraint=None, gamma_constraint=None),\n    #keras.layers.Dense(50, input_shape=(30,), activation=tf.nn.relu, kernel_initializer='glorot_normal',\n     #           kernel_regularizer=regularizers.l2(0.01), bias_initializer='zeros'),\n    #keras.layers.Dense(50, activation=tf.nn.relu, kernel_initializer='random_uniform',\n   #             kernel_regularizer=regularizers.l2(0.01), bias_initializer='zeros'),\n    #keras.layers.Dense(50, activation=tf.nn.relu, kernel_initializer='random_uniform',\n     #           kernel_regularizer=regularizers.l2(0.01), bias_initializer='zeros'),\n    keras.layers.Dense(50, activation=tf.nn.relu, kernel_initializer='random_uniform',\n               kernel_regularizer=regularizers.l2(0.01), bias_initializer='zeros'),\n    keras.layers.Dense(1, input_shape=(30,), activation=tf.nn.sigmoid, kernel_initializer='random_uniform',\n                kernel_regularizer=regularizers.l2(0.01), bias_initializer='zeros')\n])\nmodel.compile(optimizer='adam', \n              loss='binary_crossentropy',\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train, epochs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate predictions\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test[:], y_pred.round())\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\ncalulate_precision_recall(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have achieved a 99.83% accuracy, but it's predicting all as negatives, which is definitely a pickle."},{"metadata":{},"cell_type":"markdown","source":"Let's try another approach, try to add a custom loss function. Here we are going to penalise loss function if it predicts a false negative, that is predicting a positive class as negative."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Sample custom function implement to add penalise false positives\nimport keras.backend as K\ndef custom_skewed_loss(y_true, y_pred):\n    loss = (-1) * K.sum((y_true * K.log(y_pred+K.epsilon())) - 100*(y_true*(1-y_pred)) + ((1 - y_true)* K.log(1 - y_pred+K.epsilon())))\n    #loss = (1 - y_true)* K.log(1 - y_pred+K.epsilon())\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import regularizers\nmodel = keras.Sequential([\n    keras.layers.BatchNormalization(axis=-1, input_shape=(30,), momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', \n                                    beta_regularizer=None, gamma_regularizer=None, \n                                    beta_constraint=None, gamma_constraint=None),\n    #keras.layers.Dense(50, input_shape=(30,), activation=tf.nn.relu, kernel_initializer='glorot_normal',\n     #           kernel_regularizer=regularizers.l2(0.01), bias_initializer='zeros'),\n    #keras.layers.Dense(50, activation=tf.nn.relu, kernel_initializer='random_uniform',\n   #             kernel_regularizer=regularizers.l2(0.01), bias_initializer='zeros'),\n    #keras.layers.Dense(50, activation=tf.nn.relu, kernel_initializer='random_uniform',\n     #           kernel_regularizer=regularizers.l2(0.01), bias_initializer='zeros'),\n    keras.layers.Dense(50, activation=tf.nn.relu, kernel_initializer='random_uniform',\n               kernel_regularizer=regularizers.l2(0.01), bias_initializer='zeros'),\n    keras.layers.Dense(1, input_shape=(30,), activation=tf.nn.sigmoid, kernel_initializer='random_uniform',\n                kernel_regularizer=regularizers.l2(0.01), bias_initializer='zeros')\n])\nmodel.compile(optimizer='adam', \n              #loss='binary_crossentropy',\n              loss=custom_skewed_loss,\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train, y_train, epochs=4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate predictions\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test[:], y_pred.round())\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\ncalulate_precision_recall(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are getting a lower accuracy 99.80%, but overall our algorithm is performing much better. Here we can further improve recall(predecting positives) by penalising the function further, if I use coefficient of middle term as 500, I will get below result:\nAccuracy: 99.03%\nTrue Positives: 103\nTrue Negatives: 70409\nFalse Positives: 673\nFalse Negatives: 17\nPrecision: 0.1327319587628866\nRecall: 0.8583333333333333\nF1 score: 0.2299107142857143\nWe have become better at predicting true positives, but we are predicting a lot of false positives as well. Here a general rule of thumb can be to try and optimise the loss function to achieve the optimum F1 score. Although we can avoid this rule if missing a positive class is too costly."},{"metadata":{},"cell_type":"markdown","source":"Now instead of neural netwrok let's try another algorithm, XGBoost based on gradient boosting."},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBClassifier\nxgb_model = XGBClassifier()\nxgb_model.fit(X_train, y_train)\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=True, subsample=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# make predictions for test data\ny_pred = xgb_model.predict(X_test)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\ncalulate_precision_recall(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above is a pretty good result, with higher F1 score, but recall is lower as compared to neural network with custom loss function. We can use either of the two approaches depnding on our requirements."},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy\nfrom scipy.stats import multivariate_normal","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist((X[:,25]), bins=100)\n#2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19\n#20,21,22,23,24,25,26,27,28","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Z = X[:,2:26]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getMeanAndVariance(X):\n    (m,n) = X.shape\n    # initializing mu(mean) and sigma2(variance)\n    mu = np.zeros((n,1))\n    sigma2 = np.zeros((n,1))\n    # mean\n    mu = np.sum(X, axis=0)/m\n    tempSigma = X - mu\n    tempSigma2 = np.power(tempSigma, 2)\n    sigma2 = np.sum(tempSigma2, axis=0)/m\n    return (mu, sigma2)\ndef getCovariance(X):\n    return np.cov(X, rowvar=False)\ndef calculateProbabilities(X, mean, cov):\n    # var = multivariate_normal(mean, cov, allow_singular=True)\n    var = multivariate_normal(mean, cov)\n    p = np.zeros((len(X),1))\n    for i in range(len(X)):\n        p[i] = var.pdf(X[i])\n    return p","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Z_train, Z_test, y_train1, y_test1 = train_test_split(Z, Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(mean, var1) = getMeanAndVariance(Z_train)\ncov = getCovariance(Z_train)\np = calculateProbabilities(Z_train, mean, cov)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"p_val = calculateProbabilities(Z_test, mean, cov)\nprint(p_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = (p_val>0.5).astype(int)\nprint(predictions.shape)\nprint(y_train1.shape)\nprint(len(p_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Find the threshold on the basis of F1 scores."},{"metadata":{"trusted":true},"cell_type":"code","source":"def select_threshhold(y, p_val):\n    p_sort = np.sort(p_val, axis=0)\n    length = len(p_val) \n    i = 0\n    bestEpsilon = 0\n    bestF1 = 0\n    F1 = 0\n    while i < length: \n        tn = 0;\n        tp = 0;\n        fp = 0;\n        fn = 0;\n        predictions = (p_val<p_sort[i]).astype(int)\n        for j in range(len(p_val)):\n            if( y[j] == 0 and predictions[j,0] == 0):\n                tn = tn+1\n            if( y[j] == 1 and predictions[j,0] == 0):\n                fn = fn+1\n            if( y[j] == 1 and predictions[j,0] == 1):\n                tp = tp+1\n            if( y[j] == 0 and predictions[j,0] == 1):\n                fp = fp+1\n        i += 1000\n        prec=0\n        rec=0\n        #print(tn)\n        #print(tp)\n        #print(fp)\n        #print(fn)\n        #print(i)\n        if(tp+fp>0):\n            prec= tp/(tp+fp)  \n        if(tp+fn>0):\n            rec = tp/(tp+fn)\n        if(prec+rec>0):\n            F1 = 2*prec*rec/(prec+rec)\n            if F1 > bestF1:\n                bestF1 = F1\n                bestEpsilon = p_sort[i];\n    tn = 0\n    fn = 0\n    tp = 0\n    fp = 0\n    for j in range(len(p_val)):\n        if( y[j] == 0 and p_val[j,0] > bestEpsilon):\n            tn = tn+1\n        if( y[j] == 1 and p_val[j,0] > bestEpsilon):\n            fn = fn+1\n        if( y[j] == 1 and p_val[j,0] <= bestEpsilon):\n            tp = tp+1\n        if( y[j] == 0 and p_val[j,0] <= bestEpsilon):\n            fp = fp+1\n    print('True Positives: ' + str(tp))\n    print('True Negatives: ' + str(tn))\n    print('False Positives: ' + str(fp))\n    print('False Negatives: ' + str(fn))\n    print('Precision: ' + str(tp/(tp+fp)))\n    print('Recall: ' + str(tp/(tp+fn)))\n    print('F1 score: ' + str(2*prec*rec/(prec+rec)))    \n    return (bestF1, bestEpsilon)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(F1, bestEpsilon) = select_threshhold(y_test1, p_val)\nprint(F1)\nprint(bestEpsilon)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we have seen earlier simple neural network classifier, doesn't work with skewd classes. Below is another approach to solve the problem of skewed classes. It's a variant of oversampling known as SMOTE."},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"smt = SMOTE()\nX_train_s, y_train_s = smt.fit_sample(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train.shape)\nprint(X_train_s.shape)\nprint(y_train.shape)\nprint(y_train_s.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import regularizers\nmodel = keras.Sequential([\n    keras.layers.BatchNormalization(axis=-1, input_shape=(30,), momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', \n                                    beta_regularizer=None, gamma_regularizer=None, \n                                    beta_constraint=None, gamma_constraint=None),\n    keras.layers.Dense(50, activation=tf.nn.relu ,\n                       kernel_initializer='random_uniform',bias_initializer='zeros'\n                      ),\n    #keras.layers.Dense(50, activation=tf.nn.relu, kernel_initializer='random_uniform',\n     #            bias_initializer='zeros'),\n    #keras.layers.Dense(50, activation=tf.nn.relu, kernel_initializer='random_uniform',\n     #            bias_initializer='zeros'),\n    #keras.layers.Dense(50, activation=tf.nn.relu, kernel_initializer='random_uniform',\n    #             bias_initializer='zeros'),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n])\nmodel.compile(optimizer='sgd', \n              loss='binary_crossentropy',\n              #loss=custom_skewed_loss,\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train_s, y_train_s, epochs=8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate predictions\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test[:], y_pred.round())\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\ncalulate_precision_recall(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}